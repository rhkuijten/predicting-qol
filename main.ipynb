{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"751b5dfe002342898d6bcff9ba5ab3de","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Import packages"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"5b8263f212e3480b95f8105bc9e348c5","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":5317,"execution_start":1703404896816,"source_hash":null},"outputs":[],"source":["from statistics import stdev\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from lime import lime_tabular\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.calibration import calibration_curve\n","from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n","from sklearn.feature_selection import RFECV\n","from sklearn.inspection import (\n","    PartialDependenceDisplay,\n",")\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import (\n","    auc,\n","    brier_score_loss,\n","    confusion_matrix,\n","    make_scorer,\n","    roc_auc_score,\n","    roc_curve,\n",")\n","from sklearn.model_selection import (\n","    GridSearchCV,\n","    RepeatedStratifiedKFold,\n","    train_test_split,\n",")\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import PowerTransformer, StandardScaler\n","from sklearn.svm import SVC\n","from sklearn.utils import resample\n"]},{"cell_type":"markdown","metadata":{"cell_id":"389f15cfa0fd42a0b954db618fd8f378","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Define evaluation functions"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"4bbab287452f4b919e10034db746a60e","deepnote_app_is_code_hidden":true,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":99,"execution_start":1703404989819,"is_code_hidden":false,"source_hash":null},"outputs":[],"source":["def calculate_metric_statistics(scores):\n","    \"\"\"\n","    Calculate the mean and confidence interval of a list of scores.\n","\n","    Args:\n","    scores (list of float): A list of scores.\n","\n","    Returns:\n","    tuple: A tuple containing the mean score, lower bound of the confidence interval,\n","    and upper bound of the confidence interval.\n","    \"\"\"\n","    # Constants for the percentiles\n","    LOWER_PERCENTILE = 2.5\n","    UPPER_PERCENTILE = 97.5\n","\n","    # Sort the scores for percentile calculation\n","    sorted_scores = np.sort(scores)\n","\n","    # Calculate mean of the scores\n","    mean_score = round(np.mean(sorted_scores), 2)\n","\n","    # Calculate the lower and upper bounds of the confidence interval\n","    lower_ci = round(np.percentile(sorted_scores, LOWER_PERCENTILE), 2)\n","    upper_ci = round(np.percentile(sorted_scores, UPPER_PERCENTILE), 2)\n","\n","    return mean_score, lower_ci, upper_ci\n","\n","def DCA(y_true, y_pred_prob, model_dict, model_name, ax):\n","    \"\"\"\n","    Perform Decision Curve Analysis (DCA) and plot the results.\n","\n","    Args:\n","    y_true (array-like): True binary labels.\n","    y_pred_prob (array-like): Predicted probabilities.\n","    model_dict (dict): Dictionary mapping model names to display names.\n","    model_name (str): Name of the model to analyze.\n","    ax (matplotlib.axes.Axes): Axes object to plot on.\n","\n","    Returns:\n","    matplotlib.axes.Axes: The axes object with the DCA plot.\n","    \"\"\"\n","    # Define threshold group\n","    thresh_group = np.arange(0, 1, 0.05)\n","\n","    # Initialize arrays to store standardized net benefits\n","    std_nb_model = np.array([])\n","    std_nb_all_model = np.array([])\n","\n","    # Calculate the incidence of the condition in the dataset\n","    mcid_incidence = np.sum(y_true == 1)\n","    n = len(y_true)\n","\n","    for thresh in thresh_group:\n","        # Predicted classes based on the threshold\n","        y_pred = y_pred_prob >= thresh\n","\n","        # Calculate confusion matrix components\n","        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n","\n","        # Calculate net benefit\n","        nb = (tp / n) - (fp / n) * (thresh / (1 - thresh))\n","        std_nb = nb / (mcid_incidence / n) if tp != 0 and not np.isnan(nb) else 0\n","        std_nb_model = np.append(std_nb_model, std_nb)\n","\n","        # Net benefit for 'treat all' strategy\n","        _, _, _, tp_all = confusion_matrix(y_true, [1] * len(y_true)).ravel()\n","        nb_all = (tp_all / n) - (1 - (tp_all / n)) * (thresh / (1 - thresh))\n","        std_nb_all = nb_all / (mcid_incidence / n) if tp_all != 0 and not np.isnan(nb_all) else 0\n","        std_nb_all_model = np.append(std_nb_all_model, std_nb_all)\n","\n","    # Plotting the DCA curve\n","    ax.plot(thresh_group, std_nb_model, color=\"blue\", label=f\"{model_dict[model_name]}\")\n","    ax.plot(thresh_group, std_nb_all_model, color=\"grey\", label=\"Treat all\")\n","    ax.plot((0, 1), (0, 0), color=\"grey\", linestyle=\":\", label=\"Treat none\")\n","\n","    # Setting plot aesthetics\n","    ax.set_ylim(-0.15, 1.15)\n","    ax.set_xlim(0, 0.8)\n","    ax.set_xlabel(\"Threshold Probability\", size=12)\n","    ax.set_ylabel(\"Standardized Net Benefit\", size=12)\n","    ax.set_title(\"Decision Curve Analysis\", pad=25, size=15, fontweight=\"bold\")\n","    ax.grid(True)\n","    ax.spines[\"right\"].set_color((0.8, 0.8, 0.8))\n","    ax.spines[\"top\"].set_color((0.8, 0.8, 0.8))\n","    ax.legend(loc=\"upper right\")\n","\n","    return ax\n","\n","def lowess(x, y, f):\n","    \"\"\"\n","    Perform basic LOWESS (Locally Weighted Scatterplot Smoothing) with a linear model.\n","\n","    Note:\n","    - This implementation is not robust, meaning no iteration and only normally distributed errors.\n","    - It uses only linear polynomials (degree = 1).\n","\n","    Args:\n","        x (array-like): Independent variable.\n","        y (array-like): Dependent variable.\n","        f (float): Smoothing parameter, representing the fraction of data points used\n","                   for each local regression.\n","\n","    Returns:\n","        tuple: Two arrays containing the smoothed values of y and their standard errors.\n","    \"\"\"\n","    # Calculate the effective width after applying the reduction factor\n","    xwidth = f * (x.max() - x.min())\n","\n","    # Number of observations\n","    N = len(x)\n","\n","    # Ensure the data is sorted according to x\n","    order = np.argsort(x)\n","\n","    # Initialize arrays for smoothed values and their standard errors\n","    y_sm = np.zeros_like(y)\n","    y_stderr = np.zeros_like(y)\n","\n","    # Define the weighting function with clipping\n","    tricube = lambda d: np.clip((1 - np.abs(d) ** 3) ** 3, 0, 1)\n","\n","    # Perform regression for each observation\n","    for i in range(N):\n","        # Compute weights for each observation\n","        dist = np.abs((x[order][i] - x[order])) / xwidth\n","        w = tricube(dist)\n","\n","        # Form linear system with the weights\n","        A = np.stack([w, x[order] * w]).T\n","        b = w * y[order]\n","        ATA = A.T.dot(A)\n","        ATb = A.T.dot(b)\n","\n","        # Solve the linear system\n","        sol = np.linalg.solve(ATA, ATb)\n","\n","        # Predict the value for the current observation\n","        yest = A[i].dot(sol)\n","        place = order[i]\n","        y_sm[place] = yest\n","\n","        # Compute the variance\n","        sigma2 = np.sum((A.dot(sol) - y[order]) ** 2) / N\n","\n","        # Calculate the standard error for the current observation\n","        y_stderr[place] = np.sqrt(sigma2 * A[i].dot(np.linalg.inv(ATA)).dot(A[i]))\n","\n","    return y_sm, y_stderr\n","\n","def plot_cal_curve(ax, y_actual, y_pred_prob):\n","    \"\"\"\n","    Plot a calibration curve with LOWESS smoothing on the given axis.\n","\n","    Args:\n","        ax (matplotlib.axes.Axes): The matplotlib axis to plot on.\n","        y_actual (array-like): Actual binary labels.\n","        y_pred_prob (array-like): Predicted probabilities.\n","\n","    Returns:\n","        matplotlib.axes.Axes: The axis with the calibration plot.\n","    \"\"\"\n","    # Calculate the calibration curve\n","    y, x = calibration_curve(y_actual, y_pred_prob, n_bins=20)\n","\n","    # Apply LOWESS smoothing\n","    y_sm, y_std = lowess(x, y, f=1 / 3)\n","    order = np.argsort(x)\n","\n","    # Plot the smoothed calibration curve\n","    ax.plot(x[order], y_sm[order], color=\"black\", label=\"Model\", linewidth=0.5)\n","    ax.plot([0, 1], [0, 1], color=\"red\", linestyle=\"-\", linewidth=0.5)\n","\n","    # Histograms for positive and negative predicted probabilities\n","    y_pred_prob_positive = y_pred_prob[y_actual == 1]\n","    y_pred_prob_negative = y_pred_prob[y_actual == 0]\n","    weights_positive = np.ones_like(y_pred_prob_positive) / len(y_pred_prob)\n","    ax.hist(\n","        y_pred_prob_positive,\n","        weights=weights_positive,\n","        bins=np.maximum(10, 100),\n","        alpha=0.4,\n","        color=\"green\",\n","    )\n","    weights_negative = -np.ones_like(y_pred_prob_negative) / len(y_pred_prob)\n","    ax.hist(\n","        y_pred_prob_negative,\n","        weights=weights_negative,\n","        bins=np.maximum(10, 100),\n","        alpha=0.4,\n","        color=\"red\",\n","    )\n","\n","    # Fill between for confidence interval\n","    ax.fill_between(\n","        x[order],\n","        y_sm[order] - y_std[order],\n","        y_sm[order] + y_std[order],\n","        alpha=0.3,\n","        color=\"grey\",\n","    )\n","\n","    # Set plot lables and title\n","    ax.set_xlabel(\"Predicted probability\", size=12)\n","    ax.set_ylabel(\"Observed proportion\", size=12)\n","    ax.set_title(label=\"Calibration plot\", pad=25, size=15, fontweight=\"bold\")\n","    \n","    # Set plot limits and ticks\n","    ax.set_ylim(-0.1, 1.1)\n","    ax.set_yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1])\n","\n","    return ax\n","\n","\n","def test_model_performance_training(X_train, y_train, model_pipeline, model_name):\n","    \"\"\"\n","    Evaluate model performance on training data using cross-validation.\n","\n","    Args:\n","        X_train (DataFrame): Training feature dataset.\n","        y_train (Series): Training target variable.\n","        model_pipeline: Machine learning model pipeline.\n","        model_name (str): Name identifier for the model.\n","\n","    Returns:\n","        DataFrame: A DataFrame containing the model name and its performance metrics.\n","    \"\"\"\n","    \n","    model_dict = {\n","        \"rf\": \"Random Forest\",\n","        \"sgb\": \"Stochastic Gradient Boosting\",\n","        \"nn\": \"Neural Network\",\n","        \"svm\": \"Support Vector Machine\",\n","        \"plr\": \"Penalized Logistic Regression\",\n","    }\n","\n","    kf = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=42)\n","    \n","    # Initialize lists to store performance metrics\n","    auc_scores = []\n","    calibration_intercepts = []\n","    calibration_slopes = []\n","    brier_scores = []\n","\n","    # Cross-validation loop\n","    for train_index, test_index in kf.split(X_train, y_train):\n","        # Split data\n","        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n","        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n","\n","        # Fit model\n","        model_pipeline.fit(X_train_fold, y_train_fold.values.ravel())\n","\n","        # Predicted probabilities\n","        y_pred_prob = model_pipeline.predict_proba(X_test_fold)[:, 1]\n","\n","        # Calculcate and store performance metrics\n","        auc_scores.append(roc_auc_score(y_test_fold, y_pred_prob))\n","        fop, mpv = calibration_curve(\n","            y_test_fold, y_pred_prob, n_bins=20, strategy=\"uniform\"\n","        )\n","        slope, intercept = np.polyfit(mpv, fop, 1)\n","        calibration_slopes.append(slope)\n","        calibration_intercepts.append(intercept)\n","        brier_scores.append(brier_score_loss(y_test_fold, y_pred_prob))\n","\n","    # Calculate means\n","    auc_stat = calculate_metric_statistics(auc_scores)\n","    intercept_stat = calculate_metric_statistics(calibration_intercepts)\n","    slope_stat = calculate_metric_statistics(calibration_slopes)\n","    brier_stat = calculate_metric_statistics(brier_scores)\n","\n","    # Compile results\n","    results_df = pd.DataFrame(\n","        {\n","            \"model_name\": [model_dict[model_name]],\n","            \"auc_score\": [f\"{auc_stat[0]} ({auc_stat[1]} - {auc_stat[2]})\"],\n","            \"calibration_intercept\": [\n","                f\"{intercept_stat[0]} ({intercept_stat[1]} - {intercept_stat[2]})\"\n","            ],\n","            \"calibration_slope\": [\n","                f\"{slope_stat[0]} ({slope_stat[1]} - {slope_stat[2]})\"\n","            ],\n","            \"brier_score\": [f\"{brier_stat[0]} ({brier_stat[1]} - {brier_stat[2]})\"],\n","        }\n","    )\n","\n","    return results_df\n","\n","\n","def model_performance_testing(X_test,y_test,model,model_name,save=False,save_path=None,\n","                              model_to_save=None,figure=None):\n","    \"\"\"\n","    Evaluate the performance of a model on test data, with options for bootstrapping, plotting, and saving results.\n","\n","    Args:\n","        X_test (DataFrame): Test feature dataset.\n","        y_test (Series): Test target variable.\n","        model: Trained machine learning model.\n","        model_name (str): Name identifier for the model.\n","        save (bool): Whether to save plots.\n","        save_path (str): Directory path to save plots.\n","        model_to_save (list): List of models to save.\n","        figure (bool): Whether to create and display figures.\n","\n","    Returns:\n","        DataFrame, array: A DataFrame containing performance metrics and an array of predicted probabilities.\n","    \"\"\"\n","      \n","    model_dict = {\n","        \"rf\": \"Random Forest\",\n","        \"sgb\": \"Stochastic Gradient Boosting\",\n","        \"nn\": \"Neural Network\",\n","        \"svm\": \"Support Vector Machine\",\n","        \"plr\": \"Penalized Logistic Regression\",\n","    }\n","\n","    y_pred_prob = model.predict_proba(X_test)[:, 1]\n","\n","    # Parameters for bootstrap\n","    n_bootstraps = 2000\n","    n_bins = 20\n","\n","    # Initialize lists for storing perfromance metrics\n","    auc_scores = []\n","    calibration_intercepts = []\n","    calibration_slopes = []\n","    brier_scores = []\n","\n","    # Bootstrap loop\n","    for i in range(n_bootstraps):\n","        while True:\n","            X_boot, y_boot = resample(X_test, y_test, n_samples=len(y_test))\n","            # Check if the resampled set has more than one unique class\n","            if len(np.unique(y_boot)) > 1:\n","                break  # Exit the loop if the sample is valid\n","\n","        y_pred_boot = model.predict_proba(X_boot)[:, 1]\n","\n","        # Compute and score metrics\n","        auc_scores.append(roc_auc_score(y_boot, y_pred_boot))\n","        fop, mpv = calibration_curve(\n","            y_boot, y_pred_boot, n_bins=n_bins, strategy=\"uniform\"\n","        )\n","        slope, intercept = np.polyfit(mpv, fop, 1)\n","        calibration_slopes.append(slope)\n","        calibration_intercepts.append(intercept)\n","        brier_scores.append(brier_score_loss(y_boot, y_pred_boot))\n","\n","    # Calculate means of score metrics\n","    auc_stat = calculate_metric_statistics(auc_scores)\n","    intercept_stat = calculate_metric_statistics(calibration_intercepts)\n","    slope_stat = calculate_metric_statistics(calibration_slopes)\n","    brier_stat = calculate_metric_statistics(brier_scores)\n","\n","    if figure == True:\n","        # Create a figure with a single row of three subplots\n","        fig, (ax1, ax2, ax3) = plt.subplots(\n","            1, 3, figsize=(30, 10)\n","        )  # Adjust the figsize as needed\n","\n","        # Plot ROC curve\n","        fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n","        roc_auc = auc(fpr, tpr)\n","\n","        ax1.plot(\n","            fpr, tpr, label=f\"model_dict[model_name] (AUC = {roc_auc:.2f})\", color=\"red\"\n","        )\n","        ax1.grid(visible=True, which=\"major\")\n","        ax1.set_xlim([0.0, 1.0])\n","        ax1.set_ylim([0.0, 1.05])\n","        ax1.set_xlabel(\"False Positive Rate\")\n","        ax1.set_ylabel(\"True Positive Rate\")\n","        ax1.set_title(\"Receiver Operating Characteristic (ROC)\")\n","        ax1.legend(loc=\"lower right\")\n","\n","        # Plot calibration and dca curve\n","        ax2 = plot_cal_curve(ax2, y_test, y_pred_prob)\n","        ax3 = DCA(y_test, y_pred_prob, model_dict, model_name, ax3)\n","\n","        # Plot figures\n","        plt.tight_layout()\n","        plt.show()\n","\n","        if save and model_name in model_to_save:\n","            # Save the ROC plot\n","            fig_roc, ax_roc = plt.subplots(figsize=(10, 7))\n","            ax_roc.plot(\n","                fpr,\n","                tpr,\n","                label=f\"{model_dict[model_name]} (AUC = {roc_auc:.2f})\",\n","                color=\"red\",\n","            )\n","            ax_roc.grid(visible=True, which=\"major\")\n","            ax_roc.set_xlim([0.0, 1.0])\n","            ax_roc.set_ylim([0.0, 1.05])\n","            ax_roc.set_xlabel(\"False Positive Rate\")\n","            ax_roc.set_ylabel(\"True Positive Rate\")\n","            ax_roc.set_title(\"Receiver Operating Characteristic (ROC)\")\n","            ax_roc.legend(loc=\"lower right\")\n","            fig_roc.savefig(f\"{model_dict[model_name]}_ROC.png\", dpi=300)\n","\n","            # Save the Calibration plot\n","            fig_cal, ax_cal = plt.subplots(figsize=(10, 7))\n","            ax_cal = plot_cal_curve(ax_cal, y_test, y_pred_prob)\n","            fig_cal.savefig(f\"{model_dict[model_name]}_Calibration_curve.png\", dpi=300)\n","\n","            # Save the Decision Curve plot\n","            fig_dc, ax_dc = plt.subplots(figsize=(10, 7))\n","            ax_dc = DCA(y_test, y_pred_prob, model_dict, model_name, ax_dc)\n","            fig_dc.savefig(f\"{model_dict[model_name]}_Decision_Curve.png\", dpi=300)\n","\n","    # Compile results\n","    results_df = pd.DataFrame(\n","        {\n","            \"model_name\": [model_dict[model_name]],\n","            \"auc_score\": [f\"{auc_stat[0]} ({auc_stat[1]} - {auc_stat[2]})\"],\n","            \"calibration_intercept\": [\n","                f\"{intercept_stat[0]} ({intercept_stat[1]} - {intercept_stat[2]})\"\n","            ],\n","            \"calibration_slope\": [\n","                f\"{slope_stat[0]} ({slope_stat[1]} - {slope_stat[2]})\"\n","            ],\n","            \"brier_score\": [f\"{brier_stat[0]} ({brier_stat[1]} - {brier_stat[2]})\"],\n","        }\n","    )\n","\n","    return results_df, y_pred_prob"]},{"cell_type":"markdown","metadata":{"cell_id":"9306bf9c2624465e96688b6d52da2e08","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Define preprocessing methods"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"6efe809e0eb64779bc1e4e4e4b92a03c","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":12,"execution_start":1703404994031,"source_hash":null},"outputs":[],"source":["def pipeline_importance_getter(pipe):\n","    \"\"\"Get feature importances from a pipeline's named step.\"\"\"\n","    if hasattr(pipe.named_steps[\"classifier\"], \"feature_importances_\"):\n","        return pipe.named_steps[\"classifier\"].feature_importances_\n","    elif hasattr(pipe.named_steps[\"classifier\"], \"coef_\"):\n","        return abs(pipe.named_steps[\"classifier\"].coef_[0])  # For logistic regression\n","    else:\n","        raise ValueError(\n","            'The classifier does not expose \"coef_\" or \"feature_importances_\" attributes'\n","        )\n","\n","\n","class CustomPowerTransformer(BaseEstimator, TransformerMixin):\n","    def __init__(self, columns, all_features):\n","        \"\"\"\n","        Initialize the CustomPowerTransformer.\n","\n","        Parameters:\n","        - columns: list of feature names to power-transform\n","        - all_features: list of all feature names\n","        \"\"\"\n","        self.columns = columns\n","        self.all_features = all_features\n","\n","    def fit(self, X, y=None):\n","        \"\"\"\n","        Fit the transformer.\n","        Here we compute the names for columns to power-transform and fit the PowerTransformer\n","        and StandardScaler on the appropriate data.\n","        \"\"\"\n","        # Determine indices of columns to power-transform\n","        self.column_names = self.all_features[: X.shape[1]]\n","        self.column_indices = [\n","            i\n","            for i, col_name in enumerate(self.column_names)\n","            if col_name in self.columns\n","        ]\n","\n","        # Convert X to numpy array if it's a DataFrame\n","        if isinstance(X, pd.DataFrame):\n","            X = X.values\n","\n","        # Fit the power transformer to the specified columns\n","        self.power_transformer = PowerTransformer(\n","            method=\"yeo-johnson\", standardize=False\n","        )\n","        self.power_transformer.fit(X[:, self.column_indices])\n","\n","        # Transform the specified columns\n","        X_transformed = X.copy()\n","        X_transformed[:, self.column_indices] = self.power_transformer.transform(\n","            X[:, self.column_indices]\n","        )\n","\n","        # Fit the scaler to the transformed data\n","        self.scaler = StandardScaler().fit(X_transformed)\n","\n","        return self\n","\n","    def transform(self, X, y=None):\n","        \"\"\"\n","        Apply power transformation to specified columns and then scale all columns.\n","        \"\"\"\n","        # Convert X to numpy array if it's a DataFrame\n","        if isinstance(X, pd.DataFrame):\n","            X = X.values\n","\n","        # Apply power transformation to specified columns\n","        X_transformed = X.copy()\n","        X_transformed[:, self.column_indices] = self.power_transformer.transform(\n","            X[:, self.column_indices]\n","        )\n","\n","        # Scale all the columns\n","        return self.scaler.transform(X_transformed)"]},{"cell_type":"markdown","metadata":{"cell_id":"1c477a9777094876aa06d48282363ca3","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Initialize data"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"4af29f9c5669427e95289167f4a91bf5","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2091,"execution_start":1703404998966,"source_hash":null},"outputs":[],"source":["df = pd.read_excel(\"Test_QOL_Data_imputed_EQ5DIndexes.xlsx\")\n","\n","df[\"binary_ASIA\"] = df[\"ASIA\"].apply(lambda x: 0 if x <= 3 else 1)\n","df[\"binary_ASA\"] = df[\"All_ASA\"].apply(lambda x: 0 if x <= 2 else 1)\n","df[\"binary_KPS\"] = df[\"KPS\"].apply(lambda x: 0 if x >= 80 else 1)\n","df[\"Opioid\"] = df[\"Analgesic\"].apply(lambda x: 1 if x > 1 else 0)\n","\n","# Create new variable with categories\n","df[\"grouped_KPS\"] = pd.cut(\n","    df[\"KPS\"], bins=[-1, 40, 80, 100], labels=[0, 1, 2], right=True, include_lowest=True\n",")\n","\n","# Adjust for the middle bin\n","df.loc[df[\"KPS\"] == 80, \"grouped_KPS\"] = 2\n","\n","features = [\n","    \"Age\",\n","    \"BMI\",\n","    \"Gender\",\n","    \"CCI_YN\",\n","    \"binary_KPS\",\n","    \"Functional_Stat_1\",\n","    \"binary_ASIA\",\n","    \"binary_ASA\",\n","    \"Katagiri_Group\",\n","    \"Visceral\",\n","    \"Brain\",\n","    \"Liver\",\n","    \"Lung\",\n","    \"Path_Fract\",\n","    \"Nr_Skel_Met\",\n","    \"Nr_Spine_Met\",\n","    \"Nr_Ex_Spin_Foci\",\n","    \"Pre_Chem\",\n","    \"Opioid\",\n","    \"WBC\",\n","    \"Hemo\",\n","    \"Platelet\",\n","    \"Creat\",\n","    \"CHI+RT\",\n","    \"B_Mob\",\n","    \"B_Sel\",\n","    \"B_Usu\",\n","    \"B_Dis\",\n","    \"B_Anx\",\n","    \"B_Index\",\n","]\n","\n","pred_features = [\n","    \"Age\",\n","    \"BMI\",\n","    \"Gender\",\n","    \"CCI_YN\",\n","    \"binary_KPS\",\n","    \"Functional_Stat_1\",\n","    \"binary_ASIA\",\n","    \"binary_ASA\",\n","    \"Katagiri_Group\",\n","    \"Visceral\",\n","    \"Brain\",\n","    \"Liver\",\n","    \"Lung\",\n","    \"Path_Fract\",\n","    \"Nr_Skel_Met\",\n","    \"Nr_Spine_Met\",\n","    \"Nr_Ex_Spin_Foci\",\n","    \"Pre_Chem\",\n","    \"Opioid\",\n","    \"WBC\",\n","    \"Hemo\",\n","    \"Platelet\",\n","    \"Creat\",\n","    \"B_Index\",\n","]\n","\n","numerical_features = [\"Age\", \"BMI\", \"WBC\", \"Hemo\", \"Platelet\", \"Creat\", \"B_Index\"]\n","\n","# MCID recalculation\n","std_baseline = stdev(df[\"B_Index\"])\n","print(\"MCID, 0.5 of SD = \", round(0.5 * std_baseline, 2))\n","MCID = round(0.5 * std_baseline, 2)\n","df[\"MCID_Result\"] = (df[\"M3_Index\"] - df[\"B_Index\"] >= MCID).astype(int)\n","\n","# Separate the features (X) and the target variable (y)\n","X = df[features]\n","y = df[\"MCID_Result\"]\n","\n","# Perform the stratified split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, shuffle=True, stratify=y, random_state=42\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"86c0dc04f86c4397a40e545cff9fef3f","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":13,"execution_start":1703238827513,"source_hash":null},"outputs":[],"source":["df[\"MCID_Result\"].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"442530b8a3b64356815728302eb53b4d","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1371,"execution_start":1703238832902,"source_hash":null},"outputs":[],"source":["X_train[\"Test\"] = 0\n","X_test[\"Test\"] = 1\n","X_train.to_excel(\"X_train_fortable.xlsx\")\n","X_test.to_excel(\"X_test_fortable.xlsx\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"d319560d209849c6be4bc7db679e90ca","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":579,"execution_start":1703004601924,"source_hash":null},"outputs":[],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"310350395a46458cb7bf2c730ac47d5a","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":41,"execution_start":1703172328429,"source_hash":null},"outputs":[],"source":["y_train_value_counts = y_train.value_counts()\n","y_test_value_counts = y_test.value_counts()\n","\n","print(\"Training set counts:\")\n","print(y_train_value_counts)\n","\n","print(\"\\nTest set counts:\")\n","print(y_test_value_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"235cf36123f542c28639f240ba70e8c5","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":10,"execution_start":1703338672352,"source_hash":null},"outputs":[],"source":["power_and_scale_transformer_svm = CustomPowerTransformer(\n","    columns=numerical_features, all_features=pred_features\n",")\n","power_and_scale_transformer_plr = CustomPowerTransformer(\n","    columns=numerical_features, all_features=pred_features\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"7062fccba8854fa99b5d03bb243402b9","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":428055,"execution_start":1703239614427,"source_hash":null},"outputs":[],"source":["feature_pipelines = {\n","    \"rf\": Pipeline(\n","        [(\"classifier\", RandomForestClassifier(n_estimators=100, n_jobs=-1))]\n","    ),\n","    \"plr\": Pipeline(\n","        [\n","            (\"preprocessor\", power_and_scale_transformer_plr),\n","            (\n","                \"classifier\",\n","                LogisticRegression(\n","                    penalty=\"elasticnet\", solver=\"saga\", l1_ratio=0.5, max_iter=10000\n","                ),\n","            ),\n","        ]\n","    ),\n","}\n","\n","# Step 2: Use these pipelines in RFECV\n","rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=42)\n","\n","rfecv_rf_selector = RFECV(\n","    estimator=feature_pipelines[\"rf\"],\n","    step=1,\n","    cv=rskf,\n","    importance_getter=pipeline_importance_getter,\n","    n_jobs=-1,\n","    verbose=1,\n",")\n","rfecv_plr_selector = RFECV(\n","    estimator=feature_pipelines[\"plr\"],\n","    step=1,\n","    cv=rskf,\n","    importance_getter=pipeline_importance_getter,\n","    n_jobs=-1,\n","    verbose=1,\n",")\n","\n","rfecv_rf_selector.fit(X_train[pred_features], y_train)\n","rfecv_plr_selector.fit(X_train[pred_features], y_train)\n","\n","# Get optimal feature masks\n","rf_features_mask = rfecv_rf_selector.support_\n","plr_features_mask = rfecv_plr_selector.support_\n","\n","# Retrieve optimal features names\n","rf_optimal_features = [\n","    feature for feature, boolean in zip(pred_features, rf_features_mask) if boolean\n","]\n","print(rf_features_mask)\n","plr_optimal_features = [\n","    feature for feature, boolean in zip(pred_features, plr_features_mask) if boolean\n","]\n","print(plr_features_mask)\n","\n","print(\"Optimal features for Random Forest:\", rf_optimal_features)\n","print(\"Optimal features for Penalized Logistic Regression:\", plr_optimal_features)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"78649e1916ab46bebe551c52c43941e0","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":33,"execution_start":1703338679734,"source_hash":null},"outputs":[],"source":["optimal_rf = [\n","    \"Age\",\n","    \"BMI\",\n","    \"Gender\",\n","    \"CCI_YN\",\n","    \"binary_KPS\",\n","    \"binary_ASA\",\n","    \"Katagiri_Group\",\n","    \"Path_Fract\",\n","    \"Nr_Spine_Met\",\n","    \"Nr_Ex_Spin_Foci\",\n","    \"Pre_Chem\",\n","    \"Opioid\",\n","    \"WBC\",\n","    \"Hemo\",\n","    \"Platelet\",\n","    \"Creat\",\n","    \"B_Index\",\n","]\n","optimal_plr = [\"Katagiri_Group\", \"B_Index\"]"]},{"cell_type":"markdown","metadata":{"cell_id":"b88c8d87fef545b2a55dd8829980ed60","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Train model"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"a7f3a398062a4d11b5533d4778a3c77a","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":147918,"execution_start":1703338793147,"source_hash":null},"outputs":[],"source":["pipelines = {\n","    \"rf\": Pipeline([(\"classifier\", RandomForestClassifier())]),\n","    \"sgb\": Pipeline([(\"classifier\", GradientBoostingClassifier())]),\n","    \"svm\": Pipeline(\n","        [\n","            (\"scaler\", power_and_scale_transformer_svm),\n","            (\"classifier\", SVC(kernel=\"rbf\", probability=True)),\n","        ]\n","    ),\n","    \"plr\": Pipeline(\n","        [\n","            (\"preprocessor\", power_and_scale_transformer_plr),\n","            (\n","                \"classifier\",\n","                LogisticRegression(penalty=\"elasticnet\", solver=\"saga\", max_iter=10000),\n","            ),\n","        ]\n","    ),\n","}\n","\n","# Using StratifiedKFold in GridSearchCV\n","cv_method = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n","\n","# Grid Search for Hyperparameter Tuning\n","param_grid_rf = {\n","    'classifier__n_estimators': [50, 100, 200, 300, 400, 500],\n","    'classifier__max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None],\n","    'classifier__max_features': [sqrt, log2, None],\n","    'classifier__min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n","    'classifier__min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 8, 10]\n","}\n","\n","param_grid_sgb = {\n","    'classifier__loss': [\"log_loss\", \"exponential\"],\n","    'classifier__n_estimators': [25, 50, 75, 100, 150, 200],\n","    'classifier__learning_rate': [0.001, 0.01, 0.1],\n","    'classifier__max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None],\n","    'classifier__subsample': [0.5, 0.6, 0.7, 0.8, 0.9],\n","    'classifier__max_features': [sqrt, log2, None],\n","    'classifier__min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n","    'classifier__min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n","}\n","\n","param_grid_svm = {  # Non-linear SVM\n","    \"classifier__C\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1],\n","    \"classifier__gamma\": [\"scale\", \"auto\"],\n","}\n","\n","param_grid_plr = {\n","    \"classifier__C\": [0.001, 0.01, 0.01, 0.1, 1, 10, 100, 1000],\n","    \"classifier__l1_ratio\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n","}\n","\n","# Define multiple scorers\n","scorers = {\n","    \"roc_auc\": \"roc_auc\",\n","    \"neg_brier_score\": make_scorer(\n","        brier_score_loss, needs_proba=True, greater_is_better=False\n","    ),\n","}\n","\n","summary = []\n","\n","for model_name, model_pipeline in pipelines.items():\n","    if model_name == \"plr\":\n","        # Optimize hyperparameters with GridSearchCV using multiple metrics\n","        grid_search = GridSearchCV(\n","            model_pipeline,\n","            eval(f\"param_grid_{model_name}\"),\n","            cv=cv_method,\n","            scoring=scorers,\n","            refit=\"roc_auc\",\n","            verbose=1,\n","            n_jobs=-1,\n","        )\n","        grid_search.fit(X_train[optimal_plr], y_train)\n","\n","        # Get mean test scores\n","        roc_auc = grid_search.cv_results_[\"mean_test_roc_auc\"][grid_search.best_index_]\n","        brier = -grid_search.cv_results_[\"mean_test_neg_brier_score\"][\n","            grid_search.best_index_\n","        ]  # Convert back to positive Brier score\n","\n","        # Store results in a summary list\n","        summary.append(\n","            {\n","                \"Model\": model_name,\n","                \"Best Hyperparameters\": grid_search.best_params_,\n","                \"Mean ROC AUC\": round(roc_auc, 2),\n","                \"Mean Brier Score\": round(brier, 2),\n","            }\n","        )\n","\n","    else:\n","        # Optimize hyperparameters with GridSearchCV using multiple metrics\n","        grid_search = GridSearchCV(\n","            model_pipeline,\n","            eval(f\"param_grid_{model_name}\"),\n","            cv=cv_method,\n","            scoring=scorers,\n","            refit=\"roc_auc\",\n","            verbose=1,\n","            n_jobs=-1,\n","        )\n","        grid_search.fit(X_train[optimal_rf], y_train)\n","\n","        # Get mean test scores\n","        roc_auc = grid_search.cv_results_[\"mean_test_roc_auc\"][grid_search.best_index_]\n","        brier = -grid_search.cv_results_[\"mean_test_neg_brier_score\"][\n","            grid_search.best_index_\n","        ]  # Convert back to positive Brier score\n","\n","        # Store results in a summary list\n","        summary.append(\n","            {\n","                \"Model\": model_name,\n","                \"Best Hyperparameters\": grid_search.best_params_,\n","                \"Mean ROC AUC\": round(roc_auc, 2),\n","                \"Mean Brier Score\": round(brier, 2),\n","            }\n","        )\n","\n","# Convert the summary list to a DataFrame for better visualization\n","summary_df = pd.DataFrame(summary)\n","summary_df"]},{"cell_type":"markdown","metadata":{"cell_id":"4609859770c649d491abb2ac47bbddfa","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Stratified 10-fold cross-validation repeated 5 times for training performance"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"bef1bd6e978c437d9480dcd989e51a06","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":77527,"execution_start":1703339181086,"source_hash":null},"outputs":[],"source":["# Evaluate the models on training data\n","result_df = pd.DataFrame()\n","all_results = pd.DataFrame()\n","\n","for model_name, model_pipeline in pipelines.items():\n","    # Get the best parameters for the model\n","    best_params = [\n","        item[\"Best Hyperparameters\"] for item in summary if item[\"Model\"] == model_name\n","    ][0]\n","\n","    # Strip 'classifier__' prefix from the parameter names\n","    stripped_params = {\n","        key.replace(\"classifier__\", \"\"): value for key, value in best_params.items()\n","    }\n","\n","    # Update the model with the best parameters\n","    model_pipeline.named_steps[\"classifier\"].set_params(**stripped_params)\n","\n","    if model_name == \"plr\":\n","        X_train_model = X_train[optimal_plr]\n","    else:\n","        X_train_model = X_train[optimal_rf]\n","\n","    result_df = test_model_performance_training(\n","        X_train_model, y_train, model_pipeline, model_name\n","    )\n","    all_results = pd.concat([all_results, result_df], ignore_index=True)\n","\n","all_results"]},{"cell_type":"markdown","metadata":{"cell_id":"92820291918f4e5ebbd7a45960d7dd7a","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Test models"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"1b8c275220c54966a86afd425352ab41","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":72809,"execution_start":1703339905198,"source_hash":null},"outputs":[],"source":["# Store the best models\n","best_models = {}\n","\n","for model_name, model_pipeline in pipelines.items():\n","    # Get the best parameters for the model\n","    best_params = [\n","        item[\"Best Hyperparameters\"] for item in summary if item[\"Model\"] == model_name\n","    ][0]\n","\n","    # Strip 'classifier__' prefix from the parameter names\n","    stripped_params = {\n","        key.replace(\"classifier__\", \"\"): value for key, value in best_params.items()\n","    }\n","\n","    # Update the model with the best parameters\n","    model_pipeline.named_steps[\"classifier\"].set_params(**stripped_params)\n","\n","    # Determine the right feature set based on the model\n","    if model_name == \"plr\":\n","        X_train_model = X_train[optimal_plr]\n","    else:\n","        X_train_model = X_train[optimal_rf]\n","\n","    # Fit the model on the full training data\n","    model_pipeline.fit(X_train_model, y_train)\n","\n","    # Store the trained model\n","    best_models[model_name] = model_pipeline\n","\n","# Evaluate the models on test data\n","results_df = pd.DataFrame()\n","all_results = pd.DataFrame()\n","probabilities = {}\n","\n","# Evaluate the models on test data\n","for model_name, model in best_models.items():\n","    # Determine the right feature set based on the model\n","    if model_name == \"plr\":\n","        X_test_model = X_test[optimal_plr]\n","    else:\n","        X_test_model = X_test[optimal_rf]\n","\n","    # Get the performance statistics for the model\n","    figure = True\n","    save = True\n","    model_to_save = [\"plr\", \"sgb\"]\n","    results_df, model_probs = model_performance_testing(\n","        X_test_model,\n","        y_test,\n","        model=model,\n","        model_name=model_name,\n","        figure=figure,\n","        save=save,\n","        model_to_save=model_to_save,\n","    )\n","    all_results = pd.concat([all_results, results_df], ignore_index=True)\n","\n","    if model_name in [\"plr\", \"sgb\"]:\n","        probabilities[model_name] = model_probs\n","\n","all_results"]},{"cell_type":"markdown","metadata":{"cell_id":"63b472d32b984d889e8075a52e7f9069","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Null-model for Brier score"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"b4ca8cccdaf34204a8e4a739e0a512db","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":36,"execution_start":1703340090866,"source_hash":null},"outputs":[],"source":["# Extracting the actual outcomes\n","actual_outcomes = df[\"MCID_Result\"].values\n","\n","# Calculating the baseline probability\n","baseline_probability = np.mean(actual_outcomes)\n","\n","# Calculating the Brier score\n","brier_score = np.mean((baseline_probability - actual_outcomes) ** 2)\n","\n","# Displaying the results\n","print(f\"Baseline Probability: {baseline_probability:.2f}\")\n","print(f\"Brier Score: {brier_score:.2f}\")"]},{"cell_type":"markdown","metadata":{"cell_id":"a8ab84d57c9d4d3180c5d7745bd14bb1","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Plots"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"7981fb7d003e4ecb86dba75644ef39c6","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1857,"execution_start":1703340099809,"source_hash":null},"outputs":[],"source":["feature_importances = best_models[\"sgb\"].steps[-1][1].feature_importances_\n","\n","# Get sorted indices of feature importances\n","indices = np.argsort(feature_importances)[::-1]\n","\n","# Plot feature importances\n","plt.figure(figsize=(10, 8))\n","plt.title(\"Feature Importances\")\n","plt.barh(\n","    range(len(optimal_rf)), feature_importances[indices], color=\"b\", align=\"center\"\n",")\n","plt.yticks(range(len(optimal_rf)), np.array(optimal_rf)[indices])\n","plt.xlabel(\"Importance\")\n","plt.ylabel(\"Features\")\n","plt.gca().invert_yaxis()  # To display the highest importance at the top\n","plt.tight_layout()  # Adjusts plot to ensure everything fits without overlapping\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"6784a556ff444799b5b28ae79669cbb7","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1936,"execution_start":1703342303377,"source_hash":null},"outputs":[],"source":["fig, (ax1, ax2) = plt.subplots(\n","    1, 2, figsize=(10, 6)\n",")  # Adjust for the number of features and models\n","\n","\n","pd_features = [\"Katagiri_Group\", \"B_Index\"]\n","categorical_features = [\"Katagiri_Group\"]\n","\n","sgb_disp = PartialDependenceDisplay.from_estimator(\n","    best_models[\"sgb\"],\n","    X_test[optimal_rf],\n","    features=pd_features,\n","    kind=\"average\",\n","    ax=[ax1, ax2],\n","    line_kw={\"label\": \"Stochastic Gradient Boosting\"},\n",")\n","\n","plr_disp = PartialDependenceDisplay.from_estimator(\n","    best_models[\"plr\"],\n","    X_test[optimal_plr],\n","    features=pd_features,\n","    kind=\"average\",\n","    ax=[ax1, ax2],\n","    line_kw={\"label\": \"Penalized Logistic Regression\", \"color\": \"red\"},\n",")\n","\n","ax1.set_ylim(0, 1)\n","ax1.set_ylabel(\"Predicted probability\")\n","ax2.set_ylim(0, 1)\n","ax2.set_ylabel(\"Predicted probability\")\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"b0be5c7944e6481ab00c3831944ff911","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":430,"execution_start":1703341118923,"source_hash":null},"outputs":[],"source":["# Load the sgb\n","model = best_models[\"plr\"].steps[-1][1]\n","\n","predict_plr = lambda x: model.predict_proba(x).astype(float)\n","\n","# Create a LIME explainer object for the random forest model\n","explainer = lime_tabular.LimeTabularExplainer(\n","    training_data=X_train[optimal_plr].to_numpy(),\n","    feature_names=optimal_plr,\n","    class_names=[\"No MCID\", \"MCID\"],\n","    discretize_continuous=True,\n","    categorical_features=[\"Katagiri_Group\"],\n",")\n","\n","exp = explainer.explain_instance(\n","    X_test[optimal_plr].to_numpy()[2], predict_plr, num_features=6\n",")\n","exp.show_in_notebook(show_all=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"3f5daac306554a60993f2f10a9a2573f","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":382,"execution_start":1703341169607,"source_hash":null},"outputs":[],"source":["exp = explainer.explain_instance(\n","    X_test[optimal_plr].to_numpy()[3], predict_plr, num_features=6\n",")\n","exp.show_in_notebook(show_all=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"70dfc4999b194ecbb8a03d615395ffb1","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1716,"execution_start":1703341200360,"source_hash":null},"outputs":[],"source":["cases = [2, 3]\n","for case in cases:\n","    exp = explainer.explain_instance(\n","        X_test[optimal_plr].to_numpy()[case], predict_plr, num_features=6\n","    )\n","    test = exp.as_pyplot_figure()"]}],"metadata":{"deepnote_execution_queue":[],"deepnote_notebook_id":"ffdfbbcb0f4d4572ba194c59aa27a354","language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
